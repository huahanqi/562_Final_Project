{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common:\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cfcaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.shape[0] - full_data_cln.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb8306",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "file_path = \"./hotel_bookings.csv\"\n",
    "full_data = pd.read_csv(file_path)\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for missing values\n",
    "full_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f14454",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace missing values:\n",
    "# only 3 columns have missing values: country; deposit_type; company\n",
    "replace_dict = {\"children:\": 0.0, \"country\": \"Unknown\", \"agent\": 0, \"company\": 0}\n",
    "full_data_cln = full_data.fillna(replace_dict)\n",
    "\n",
    "# \"meal\" contains values \"Undefined\", which is equal to SC.\n",
    "full_data_cln[\"meal\"].replace(\"Undefined\", \"SC\", inplace=True)\n",
    "\n",
    "# Some rows contain entreis with 0 adults, 0 children and 0 babies. \n",
    "# drop entries with no guests.\n",
    "zero_guests = list(full_data_cln.loc[full_data_cln[\"adults\"]\n",
    "                   + full_data_cln[\"children\"]\n",
    "                   + full_data_cln[\"babies\"]==0].index)\n",
    "full_data_cln.drop(full_data_cln.index[zero_guests], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after cleaning, there is 119210 samples\n",
    "full_data_cln.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a417fa",
   "metadata": {},
   "source": [
    "# Cancellation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d151ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel_corr = full_data_cln.corr()[\"is_canceled\"]\n",
    "corr = cancel_corr.abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cdf31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = corr[1:]\n",
    "xs = ['lead_time', 'total_of_special_requests', 'required_car_parking_spaces',\\\n",
    "     'booking_changes', 'previous_cancellations', 'is_repeated_guest', \\\n",
    "     'company', 'adults', 'previous_bookings_not_canceled', 'days_in_waiting_list',\\\n",
    "     'agent', 'adr', 'babies', 'stays_in_week_nights', 'arrival_date_year', \\\n",
    "     'arrival_date_week_number', 'arrival_date_day_of_month', 'children', 'stays_in_weekend_nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfcc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(xs, ys)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel(\"column name\")\n",
    "plt.ylabel(\"correlation\")\n",
    "plt.title(\"Correlation with Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e3ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel_counts = full_data_cln['is_canceled'].value_counts()\n",
    "cancel_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7339a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# naive accuracy\n",
    "plt.bar([\"not cancelled\", \"cancelled\"], cancel_counts)\n",
    "plt.ylabel(\"counts\")\n",
    "plt.title(\"Class Distribution for Cancellation Prediction\")\n",
    "plt.show()\n",
    "cancel_counts[0] / (cancel_counts[0] + cancel_counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose the 10 most useful features based on correlation\n",
    "\n",
    "# all features are numerical\n",
    "full_feature_list = [\"lead_time\", \"total_of_special_requests\", \\\n",
    "                    \"required_car_parking_spaces\", \"booking_changes\",\\\n",
    "                    \"previous_cancellations\", \"is_repeated_guest\",\\\n",
    "                    \"agent\", \"adults\", \"previous_bookings_not_canceled\",\\\n",
    "                    \"days_in_waiting_list\"]\n",
    "\n",
    "X = full_data_cln.drop([\"is_canceled\"], axis=1)[full_feature_list]\n",
    "y = full_data_cln[\"is_canceled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b84b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_folds = 5 # 5 fold cross validation = 80% train, 20% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_name, X, y, num_folds):    \n",
    "    clf = make_pipeline(StandardScaler(), model)\n",
    "    scores = cross_val_score(clf, X, y, cv=num_folds, scoring='accuracy', n_jobs=-1)\n",
    "    print(\"{} performance: {:.2f} +- {:.2f}\".format(model_name, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model - Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model_name = \"Gaussian Naive Bayes\"\n",
    "\n",
    "# train\n",
    "train(model, model_name, X, y, num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45059ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# init model - Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model_name = \"Logistic Regression\"\n",
    "\n",
    "# train\n",
    "train(model, model_name, X, y, num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model - XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model_name = \"XGBoost\"\n",
    "\n",
    "# train\n",
    "train(model, model_name, X, y, num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model - simple neural network\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(input_dim, hid_dim) \n",
    "        self.layer_2 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.layer_out = nn.Linear(hid_dim, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hid_dim)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hid_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer_1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import ipdb\n",
    "    \n",
    "def evaluate(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    y_pred_list = []\n",
    "    y_gt_list = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.float(), labels.float()\n",
    "            labels = labels.unsqueeze(-1)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(inputs)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.squeeze()).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on test set: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e323d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_ann(X, y, num_folds, lr = 0.01, hid_dim = 64, loss_func = nn.BCELoss(), run_cv=False):\n",
    "    \n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "    \n",
    "    # grid search \n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)):\n",
    "        # Print\n",
    "        print(f'FOLD {fold}')\n",
    "\n",
    "        # init data\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        X_train = X.to_numpy()[train_ids]\n",
    "        y_train = y.to_numpy()[train_ids]\n",
    "        X_test = X.to_numpy()[test_ids]\n",
    "        y_test = y.to_numpy()[test_ids]\n",
    "        # normalize data\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train_transformed = scaler.transform(X_train)\n",
    "        X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            train_data.append([X_train_transformed[i], y_train[i]])\n",
    "        for i in range(len(X_test)):\n",
    "            test_data.append([X_test_transformed[i], y_test[i]])\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=512)\n",
    "        testloader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=512)\n",
    "\n",
    "        # init model\n",
    "        simple_ann = SimpleNet(X.shape[1], hid_dim)\n",
    "\n",
    "        # init training\n",
    "        optimizer = torch.optim.SGD(simple_ann.parameters(), lr=lr, momentum=0.9)\n",
    "        criterion = loss_func\n",
    "\n",
    "        # training\n",
    "        for epoch in range(num_epoch):  \n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.float(), labels.float()\n",
    "                labels = labels.unsqueeze(-1)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = simple_ann(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                if i%200 == 199:\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    num_correct = (predicted == labels.squeeze()).sum().item()\n",
    "                    print(loss.item(), num_correct / 512.0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            if epoch % 10 == 9:\n",
    "                print('[epoch %d] loss: %.3f' %\n",
    "                      (epoch + 1, running_loss / len(trainloader)))\n",
    "        simple_ann.eval()\n",
    "        evaluate(simple_ann, trainloader)\n",
    "        evaluate(simple_ann, testloader)\n",
    "        simple_ann.train()\n",
    "        if not run_cv:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.1, 0.03, 0.01, 0.003, 0.001]\n",
    "hid_dim_list = [256, 64, 16]\n",
    "loss_list = [nn.BCELoss()]\n",
    "\n",
    "num_epoch = 50\n",
    "\n",
    "for loss in loss_list:\n",
    "    for hid_dim in hid_dim_list: \n",
    "        for lr in lr_list:\n",
    "            print(lr, hid_dim, loss)\n",
    "            train_ann(X, y, 5, lr, hid_dim, loss, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
